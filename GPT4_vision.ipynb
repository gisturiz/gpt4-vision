{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8qu3BRupUN6V1Vvg9HfHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gisturiz/gpt4-vision/blob/main/GPT4_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies (ignore pip dependency errors for cohere and tiktoken)"
      ],
      "metadata": {
        "id": "Sx502rkYpT6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-yevxuYnfWP"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytube openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set OpenAI API Key as an environment variable. If you don't have one yet, you can get it here: https://platform.openai.com/"
      ],
      "metadata": {
        "id": "-EI9Qxn2pXZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY="
      ],
      "metadata": {
        "id": "Ai61FDaLpJK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all of the dependencies we will be using"
      ],
      "metadata": {
        "id": "GClaftf8poRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image, Audio\n",
        "\n",
        "import cv2\n",
        "import base64\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from pytube import YouTube\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "_kVpwf-_o4Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading & Video title helper functions"
      ],
      "metadata": {
        "id": "IT7qW_O9sqH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_video_title(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Formats the video title by replacing spaces with hyphens, converting to lowercase,\n",
        "    and removing special characters including commas, hash symbols, and hyphens.\n",
        "\n",
        "    Args:\n",
        "        title (str): The original video title.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted video title.\n",
        "    \"\"\"\n",
        "    title = title.lower()\n",
        "    title = re.sub(r'[^\\w\\s-]', '', title)\n",
        "    title = re.sub(r'[-\\s]+', '-', title)\n",
        "    return title\n",
        "\n",
        "def download_youtube_video(url: str, output_path: str = '.'):\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video to a specified output path.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the YouTube video.\n",
        "        output_path (str): Path where the video will be saved. Defaults to the current directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        yt = YouTube(url)\n",
        "        video_stream = yt.streams.filter(\n",
        "            progressive=True,\n",
        "            file_extension='mp4').order_by('resolution').desc().first()\n",
        "\n",
        "        if video_stream:\n",
        "            formatted_title = format_video_title(yt.title) + '.mp4'\n",
        "            video_stream.download(output_path=output_path, filename=formatted_title)\n",
        "            print(f\"Video downloaded successfully: {formatted_title}\")\n",
        "            return formatted_title\n",
        "        else:\n",
        "            print(\"No suitable video stream found.\")\n",
        "    except Exception as e:\n",
        "         print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "8o8_cRdipQq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert here the URL of the [Youtube](https://www.youtube.com/) video you'd like to download"
      ],
      "metadata": {
        "id": "4VPG9s90s1qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YT_URL='https://www.youtube.com/watch?v=INcW26-iyqU'"
      ],
      "metadata": {
        "id": "R3avHGO_qRVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the video"
      ],
      "metadata": {
        "id": "ZDLvlQcytDk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = download_youtube_video(url=YT_URL)"
      ],
      "metadata": {
        "id": "eD-sW-vLqo98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing video into frames, this will allow us to pass frames (images) to our GPT4 Vision endpoint."
      ],
      "metadata": {
        "id": "ZFw6NkqwtKDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "base64Frames = []\n",
        "while video.isOpened():\n",
        "    success, frame = video.read()\n",
        "    if not success:\n",
        "        break\n",
        "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
        "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
        "\n",
        "video.release()\n",
        "print(len(base64Frames), \"frames read.\")"
      ],
      "metadata": {
        "id": "S5uZjZJDqutD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optional* - here you can cut the length of your video by slicing the base64Frames list. This could be necessary as GPT4 Vision only has a 10,000 token per minute limit during preview."
      ],
      "metadata": {
        "id": "GL0kqHW3tYX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base64Frames = base64Frames[475:775]\n",
        "print(len(base64Frames), \"frames read.\")"
      ],
      "metadata": {
        "id": "vDrdf0N-5CVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can display the frames withing our base64Frames list to verify it was downloaded appropriately."
      ],
      "metadata": {
        "id": "77mp50StuAoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_handle = display(None, display_id=True)\n",
        "for img in base64Frames:\n",
        "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
        "    time.sleep(0.025)"
      ],
      "metadata": {
        "id": "d19wEnZorXg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will pass out base64Frames list along with our instructions to GPT4 Vision to output the text we want (narration, description, etc.)\n",
        "Note that I have included several variables to define to make the prompt better:\n",
        "\n",
        "\n",
        "*   *frame_step* - how often you want GPT4 to look at the list of frames to see an image (note that this will affect the amount of tokens per minute limit)\n",
        "*   *video_length* - let the prompt know how long, more or less, the text should be for a given video lenght (if you want narration, if not, you can delete this part of the prompt.)\n",
        "*   *prompt_instructions* - let GPT4 Vision what you would like as a response and include any other instructions for the model. Feel free to tinker here to get the best response."
      ],
      "metadata": {
        "id": "oZLB3rM_uJjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_step = 35\n",
        "video_length = 30\n",
        "prompt_instructions = f\"These are frames of a video. Create a short voiceover script in the style of David Attenborough. Only include the narration. Make the text succint so that it can be read out in {video_length} seconds.\"\n",
        "\n",
        "PROMPT_MESSAGES = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [prompt_instructions, *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::frame_step]),\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "params = {\n",
        "    \"model\": \"gpt-4-vision-preview\",\n",
        "    \"messages\": PROMPT_MESSAGES,\n",
        "    \"max_tokens\": 500,\n",
        "}\n",
        "\n",
        "result = client.chat.completions.create(**params)\n",
        "print(result.choices[0].message.content)"
      ],
      "metadata": {
        "id": "eCKJ1ZolsMvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Voice Over"
      ],
      "metadata": {
        "id": "tDJZsTZsah2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.audio.speech.create(\n",
        "    model=\"tts-1-hd\",\n",
        "    voice=\"alloy\",\n",
        "    input=result.choices[0].message.content,\n",
        ")"
      ],
      "metadata": {
        "id": "a2i1uuftGX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Save audio to mp3 file and play it to verify"
      ],
      "metadata": {
        "id": "OtAKZpIgaw5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.stream_to_file(\"output.mp3\")\n",
        "Audio(\"output.mp3\")"
      ],
      "metadata": {
        "id": "pXGX7Yx4hXdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the video and audio lengths"
      ],
      "metadata": {
        "id": "8FkASIFPbe3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i {video_path} 2>&1 | grep \"Duration\"\n",
        "!ffmpeg -i output.mp3 2>&1 | grep \"Duration\""
      ],
      "metadata": {
        "id": "BWnvSBrWVg0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine video and generated audio to overlay the newly created narration over the original video. *Note, there are some settings I tweaked for my particulat overlay, like lowering the original video's volume by half so the narration could be clearly heard. Here is [ffmpeg documentation](https://ffmpeg.org/documentation.html) should you want to make you're own changes."
      ],
      "metadata": {
        "id": "3OUDi2Pzb263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i {video_path} -i output.mp3 -filter_complex \"[0:a]volume=0.5[a0];[a0][1:a]amix=inputs=2:duration=longest[a]\" -map 0:v -map \"[a]\" -c:v copy -c:a aac -strict experimental output.mp4"
      ],
      "metadata": {
        "id": "dci2sCxgh9Sl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}